"""
Technical Details Page for Streamlit Dashboard
============================================

Comprehensive technical documentation including model details,
architecture, deployment considerations, and implementation guide.
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json

def show_technical_details():
    """Display comprehensive technical documentation."""
    
    st.markdown("## ðŸ“‹ Technical Documentation")
    st.markdown("Comprehensive technical implementation details and specifications")
    
    # Technical overview tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ðŸ—ï¸ Architecture", "ðŸ¤– Model Details", "ðŸ’» Implementation", 
        "ðŸš€ Deployment", "ðŸ“Š API Documentation"
    ])
    
    with tab1:
        show_architecture()
    
    with tab2:
        show_model_details()
    
    with tab3:
        show_implementation_guide()
    
    with tab4:
        show_deployment_guide()
    
    with tab5:
        show_api_documentation()

def show_architecture():
    """Show system architecture details."""
    
    st.markdown("### ðŸ—ï¸ System Architecture Overview")
    
    # Architecture diagram (text-based)
    st.markdown("""
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    OLIST ML PREDICTION SYSTEM                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DATA LAYER  â”‚    â”‚ MODEL LAYER  â”‚    â”‚   API LAYER  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚   Raw Data  â”‚    â”‚  Feature Pipeline â”‚    â”‚ REST API  â”‚
    â”‚ - Orders    â”‚    â”‚ - Preprocessor    â”‚    â”‚ - Predict â”‚
    â”‚ - Reviews   â”‚    â”‚ - Engineer        â”‚    â”‚ - Explain â”‚
    â”‚ - Customers â”‚    â”‚ - Scaler          â”‚    â”‚ - Monitor â”‚
    â”‚ - Products  â”‚    â”‚                   â”‚    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Data Store  â”‚    â”‚   ML Models       â”‚    â”‚ Frontend  â”‚
    â”‚ - PostgreSQLâ”‚    â”‚ - XGBoost         â”‚    â”‚ - Streamlitâ”‚
    â”‚ - Redis     â”‚    â”‚ - Random Forest   â”‚    â”‚ - Dashboardâ”‚
    â”‚ - S3 Bucket â”‚    â”‚ - LightGBM        â”‚    â”‚ - Reports â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    INFRASTRUCTURE LAYER                         â”‚
    â”‚  - Kubernetes Cluster                                          â”‚
    â”‚  - Docker Containers                                           â”‚
    â”‚  - Monitoring (Prometheus + Grafana)                          â”‚
    â”‚  - Logging (ELK Stack)                                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)
    
    # Architecture components
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ðŸ—„ï¸ Data Architecture")
        
        data_components = {
            'Component': [
                'PostgreSQL', 'Redis Cache', 'S3 Storage', 
                'Kafka Streams', 'Data Warehouse', 'Feature Store'
            ],
            'Purpose': [
                'Transactional data storage',
                'Real-time caching',
                'Model artifacts & logs',
                'Real-time data streaming',
                'Analytics & reporting',
                'Feature management'
            ],
            'Technology': [
                'PostgreSQL 13+',
                'Redis 6+',
                'AWS S3',
                'Apache Kafka',
                'BigQuery/Snowflake',
                'Feast/MLflow'
            ]
        }
        
        data_df = pd.DataFrame(data_components)
        st.dataframe(data_df, use_container_width=True)
    
    with col2:
        st.markdown("#### ðŸ¤– ML Pipeline Architecture")
        
        ml_components = {
            'Stage': [
                'Data Ingestion', 'Feature Engineering', 'Model Training',
                'Model Validation', 'Model Deployment', 'Monitoring'
            ],
            'Tools': [
                'Apache Airflow',
                'Pandas + Custom Pipeline',
                'MLflow + Ray',
                'Great Expectations',
                'Kubernetes + Seldon',
                'Prometheus + Custom'
            ],
            'Frequency': [
                'Real-time', 'Batch (Daily)', 'Weekly',
                'Per Deployment', 'On-demand', 'Continuous'
            ]
        }
        
        ml_df = pd.DataFrame(ml_components)
        st.dataframe(ml_df, use_container_width=True)
    
    # Technology stack
    st.markdown("---")
    st.markdown("### ðŸ’» Technology Stack")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        #### ðŸ Core Technologies
        
        **Programming Languages:**
        - Python 3.9+ (Primary)
        - SQL (Data queries)
        - JavaScript (Frontend)
        
        **ML Frameworks:**
        - Scikit-learn 1.0+
        - XGBoost 1.6+
        - LightGBM 3.3+
        - Pandas 1.5+
        - NumPy 1.23+
        """)
    
    with col2:
        st.markdown("""
        #### â˜ï¸ Infrastructure
        
        **Container & Orchestration:**
        - Docker 20+
        - Kubernetes 1.24+
        - Helm Charts
        
        **Cloud Services:**
        - AWS/GCP/Azure
        - Load Balancers
        - Auto-scaling Groups
        - Managed Databases
        """)
    
    with col3:
        st.markdown("""
        #### ðŸ“Š Monitoring & Ops
        
        **Observability:**
        - Prometheus (Metrics)
        - Grafana (Dashboards)
        - ELK Stack (Logging)
        - Jaeger (Tracing)
        
        **ML Operations:**
        - MLflow (Experiments)
        - Great Expectations (Quality)
        - Evidently AI (Monitoring)
        """)
    
    # Scalability considerations
    st.markdown("---")
    st.markdown("### ðŸ“ˆ Scalability & Performance")
    
    scalability_metrics = {
        'Component': [
            'Prediction API', 'Feature Pipeline', 'Model Training',
            'Data Storage', 'Real-time Processing', 'Batch Processing'
        ],
        'Current Capacity': [
            '1,000 req/sec', '10M records/day', '100M samples',
            '1TB transactional', '10K events/sec', '50M records/hour'
        ],
        'Target Capacity': [
            '10,000 req/sec', '100M records/day', '1B samples',
            '10TB transactional', '100K events/sec', '500M records/hour'
        ],
        'Scaling Strategy': [
            'Horizontal + Load Balancer',
            'Parallel Processing',
            'Distributed Training',
            'Sharding + Replication',
            'Kafka Partitioning',
            'Spark Cluster'
        ]
    }
    
    scalability_df = pd.DataFrame(scalability_metrics)
    st.dataframe(scalability_df, use_container_width=True)

def show_model_details():
    """Show detailed model specifications."""
    
    st.markdown("### ðŸ¤– Model Technical Specifications")
    
    # Model comparison table
    model_specs = {
        'Model': ['XGBoost', 'Random Forest', 'Logistic Regression', 'LightGBM'],
        'Algorithm Type': ['Gradient Boosting', 'Ensemble', 'Linear', 'Gradient Boosting'],
        'Training Time': ['45.2s', '67.8s', '12.1s', '38.9s'],
        'Prediction Time': ['2.3ms', '8.7ms', '0.8ms', '2.1ms'],
        'Memory Usage': ['156MB', '234MB', '45MB', '142MB'],
        'Features Used': [38, 38, 38, 38],
        'Max Depth': [6, 10, 'N/A', 6],
        'Regularization': ['L1+L2', 'Bootstrap', 'L2', 'L1+L2']
    }
    
    specs_df = pd.DataFrame(model_specs)
    st.dataframe(specs_df, use_container_width=True)
    
    # Hyperparameter details
    st.markdown("---")
    st.markdown("### âš™ï¸ Hyperparameter Configuration")
    
    selected_model = st.selectbox("Select model for detailed hyperparameters:", 
                                 ['XGBoost', 'Random Forest', 'Logistic Regression', 'LightGBM'])
    
    hyperparams = {
        'XGBoost': {
            'objective': 'binary:logistic',
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1,
            'eval_metric': 'auc'
        },
        'Random Forest': {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt',
            'class_weight': 'balanced',
            'bootstrap': True,
            'random_state': 42,
            'n_jobs': -1,
            'oob_score': True
        },
        'Logistic Regression': {
            'penalty': 'l2',
            'C': 1.0,
            'class_weight': 'balanced',
            'solver': 'lbfgs',
            'max_iter': 1000,
            'random_state': 42,
            'n_jobs': -1,
            'warm_start': False
        },
        'LightGBM': {
            'objective': 'binary',
            'boosting_type': 'gbdt',
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1,
            'metric': 'auc'
        }
    }
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.json(hyperparams[selected_model])
    
    with col2:
        st.markdown(f"""
        #### ðŸ“ {selected_model} Configuration Notes
        """)
        
        if selected_model == 'XGBoost':
            st.markdown("""
            **Key Configuration Decisions:**
            - `max_depth=6`: Prevents overfitting while capturing interactions
            - `learning_rate=0.1`: Balanced learning speed vs accuracy
            - `subsample=0.8`: Reduces overfitting via row sampling
            - `colsample_bytree=0.8`: Feature sampling for regularization
            - `reg_alpha/lambda`: L1 and L2 regularization
            
            **Optimization Notes:**
            - Early stopping used with 50 rounds patience
            - AUC-ROC as evaluation metric for imbalanced data
            - Tree-based method handles mixed data types well
            """)
        elif selected_model == 'Random Forest':
            st.markdown("""
            **Key Configuration Decisions:**
            - `n_estimators=100`: Good balance of performance vs speed
            - `max_depth=10`: Deeper trees than XGBoost for ensemble
            - `min_samples_split=5`: Prevents overfitting
            - `class_weight='balanced'`: Handles class imbalance
            - `bootstrap=True`: Uses bootstrap sampling
            
            **Optimization Notes:**
            - Out-of-bag score used for validation
            - Feature importance from mean decrease impurity
            - Parallel training across all cores
            """)
        elif selected_model == 'Logistic Regression':
            st.markdown("""
            **Key Configuration Decisions:**
            - `penalty='l2'`: Ridge regularization
            - `C=1.0`: Regularization strength
            - `class_weight='balanced'`: Addresses class imbalance
            - `solver='lbfgs'`: Efficient for small datasets
            - `max_iter=1000`: Ensures convergence
            
            **Optimization Notes:**
            - Feature scaling applied via StandardScaler
            - Coefficients provide direct interpretability
            - Fast training and prediction times
            """)
        else:  # LightGBM
            st.markdown("""
            **Key Configuration Decisions:**
            - `boosting_type='gbdt'`: Standard gradient boosting
            - Similar hyperparameters to XGBoost for comparison
            - Optimized for speed and memory efficiency
            - Native categorical feature support
            
            **Optimization Notes:**
            - Faster training than XGBoost
            - Lower memory usage
            - Histogram-based algorithm
            """)
    
    # Feature engineering pipeline
    st.markdown("---")
    st.markdown("### ðŸ”§ Feature Engineering Pipeline")
    
    pipeline_steps = {
        'Step': [
            '1. Data Loading',
            '2. Data Validation', 
            '3. Data Cleaning',
            '4. Feature Aggregation',
            '5. Feature Engineering',
            '6. Feature Selection',
            '7. Feature Scaling',
            '8. Final Validation'
        ],
        'Description': [
            'Load and merge 9 datasets',
            'Check data quality and integrity',
            'Handle missing values via exclusion',
            'Aggregate order-level features',
            'Create 38+ engineered features',
            'Remove low-importance features',
            'StandardScaler for numeric features',
            'Final dataset validation'
        ],
        'Input Size': [
            '9 tables', '99,441 orders', '94,750 orders',
            '94,750 orders', '94,750 orders', '94,750 orders',
            '94,750 orders', '94,750 orders'
        ],
        'Output Size': [
            '9 tables', '99,441 orders', '94,750 orders',
            '94,750 orders', '94,750 Ã— 38', '94,750 Ã— 38',
            '94,750 Ã— 38', '94,750 Ã— 38'
        ]
    }
    
    pipeline_df = pd.DataFrame(pipeline_steps)
    st.dataframe(pipeline_df, use_container_width=True)
    
    # Model validation strategy
    st.markdown("---")
    st.markdown("### âœ… Model Validation Strategy")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        #### ðŸ“Š Validation Methodology
        
        **Cross-Validation:**
        - 5-fold stratified cross-validation
        - Maintains class balance across folds
        - Repeated 3 times for stability assessment
        
        **Train-Test Split:**
        - 80% training, 20% testing
        - Stratified split preserves class distribution
        - Temporal ordering respected (no data leakage)
        
        **Evaluation Metrics:**
        - Primary: AUC-ROC (handles imbalance)
        - Secondary: Accuracy, Precision, Recall, F1
        - Business: Cost-weighted metrics
        """)
    
    with col2:
        st.markdown("""
        #### ðŸ” Validation Checks
        
        **Data Quality Validation:**
        - Schema validation with Great Expectations
        - Statistical distribution checks
        - Missing value pattern analysis
        
        **Model Quality Validation:**
        - Performance threshold checks
        - Feature importance stability
        - Prediction distribution analysis
        
        **Business Logic Validation:**
        - Prediction reasonableness checks
        - Edge case handling verification
        - Bias and fairness assessment
        """)

def show_implementation_guide():
    """Show implementation guide and code examples."""
    
    st.markdown("### ðŸ’» Implementation Guide")
    
    # Code structure
    st.markdown("#### ðŸ“ Project Structure")
    
    st.code("""
    olist-review-prediction/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.py                 # Configuration settings
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”œâ”€â”€ loader.py             # Data loading utilities
    â”‚   â”‚   â”œâ”€â”€ quality.py            # Data quality checks
    â”‚   â”‚   â””â”€â”€ preprocessor.py       # Data preprocessing
    â”‚   â”œâ”€â”€ features/
    â”‚   â”‚   â””â”€â”€ engineer.py           # Feature engineering
    â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â””â”€â”€ trainer.py            # Model training
    â”‚   â”œâ”€â”€ evaluation/
    â”‚   â”‚   â””â”€â”€ evaluator.py          # Model evaluation
    â”‚   â”œâ”€â”€ visualization/
    â”‚   â”‚   â”œâ”€â”€ report_generator.py   # HTML reports
    â”‚   â”‚   â””â”€â”€ chart_generator.py    # Interactive charts
    â”‚   â””â”€â”€ utils/
    â”‚       â””â”€â”€ logger.py             # Logging utilities
    â”œâ”€â”€ streamlit_pages/              # Streamlit dashboard pages
    â”œâ”€â”€ notebooks/                    # Jupyter notebooks
    â”œâ”€â”€ tests/                        # Unit tests
    â”œâ”€â”€ docker/                       # Docker configurations
    â”œâ”€â”€ kubernetes/                   # K8s deployment files
    â”œâ”€â”€ requirements.txt              # Python dependencies
    â”œâ”€â”€ main.py                       # Main execution script
    â””â”€â”€ streamlit_app.py             # Streamlit app entry point
    """, language='text')
    
    # Installation guide
    st.markdown("---")
    st.markdown("#### ðŸš€ Installation & Setup")
    
    tab1, tab2, tab3 = st.tabs(["ðŸ Local Setup", "ðŸ³ Docker", "â˜¸ï¸ Kubernetes"])
    
    with tab1:
        st.markdown("**Prerequisites:**")
        st.code("""
        # Python 3.9+
        python --version
        
        # Git
        git --version
        
        # Optional: Virtual environment
        python -m venv venv
        source venv/bin/activate  # On Windows: venv\\Scripts\\activate
        """, language='bash')
        
        st.markdown("**Installation Steps:**")
        st.code("""
        # 1. Clone repository
        git clone https://github.com/your-org/olist-review-prediction.git
        cd olist-review-prediction
        
        # 2. Install dependencies
        pip install -r requirements.txt
        
        # 3. Set up environment variables
        cp .env.example .env
        # Edit .env with your configuration
        
        # 4. Download and prepare data
        # Place Olist dataset files in data/ directory
        
        # 5. Run the pipeline
        python main.py
        
        # 6. Start Streamlit dashboard
        streamlit run streamlit_app.py
        """, language='bash')
    
    with tab2:
        st.markdown("**Docker Setup:**")
        st.code("""
        # Build Docker image
        docker build -t olist-ml-app .
        
        # Run container
        docker run -p 8501:8501 olist-ml-app
        
        # Or use docker-compose
        docker-compose up -d
        """, language='bash')
        
        st.markdown("**Dockerfile:**")
        st.code("""
        FROM python:3.9-slim
        
        WORKDIR /app
        
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        
        COPY . .
        
        EXPOSE 8501
        
        HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health
        
        ENTRYPOINT ["streamlit", "run", "streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
        """, language='dockerfile')
    
    with tab3:
        st.markdown("**Kubernetes Deployment:**")
        st.code("""
        # Apply namespace
        kubectl apply -f kubernetes/namespace.yaml
        
        # Deploy application
        kubectl apply -f kubernetes/deployment.yaml
        kubectl apply -f kubernetes/service.yaml
        kubectl apply -f kubernetes/ingress.yaml
        
        # Check status
        kubectl get pods -n olist-ml
        kubectl get services -n olist-ml
        """, language='bash')
        
        st.markdown("**Sample Deployment YAML:**")
        st.code("""
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: olist-ml-app
          namespace: olist-ml
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: olist-ml-app
          template:
            metadata:
              labels:
                app: olist-ml-app
            spec:
              containers:
              - name: app
                image: olist-ml-app:latest
                ports:
                - containerPort: 8501
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "250m"
                  limits:
                    memory: "1Gi"
                    cpu: "500m"
                env:
                - name: ENV
                  value: "production"
        """, language='yaml')
    
    # Configuration guide
    st.markdown("---")
    st.markdown("#### âš™ï¸ Configuration Guide")
    
    st.markdown("**Environment Variables:**")
    st.code("""
    # Application Settings
    ENV=production
    DEBUG=false
    LOG_LEVEL=INFO
    
    # Data Sources
    DATA_PATH=/app/data
    MODEL_PATH=/app/models
    OUTPUT_PATH=/app/output
    
    # Database Configuration
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=olist_ml
    DB_USER=ml_user
    DB_PASSWORD=secure_password
    
    # ML Configuration
    MODEL_VERSION=v1.0.0
    PREDICTION_THRESHOLD=0.5
    BATCH_SIZE=1000
    
    # Monitoring
    PROMETHEUS_PORT=9090
    GRAFANA_PORT=3000
    """, language='bash')

def show_deployment_guide():
    """Show deployment and operations guide."""
    
    st.markdown("### ðŸš€ Deployment & Operations")
    
    # Deployment strategies
    st.markdown("#### ðŸ“¦ Deployment Strategies")
    
    deployment_options = {
        'Strategy': ['Blue-Green', 'Rolling Update', 'Canary', 'A/B Testing'],
        'Use Case': [
            'Zero-downtime major updates',
            'Gradual rollout with monitoring',
            'Risk mitigation for new models',
            'Model performance comparison'
        ],
        'Complexity': ['Medium', 'Low', 'High', 'High'],
        'Risk': ['Low', 'Medium', 'Low', 'Medium'],
        'Rollback Time': ['Instant', '5-10 min', 'Instant', 'Manual']
    }
    
    deployment_df = pd.DataFrame(deployment_options)
    st.dataframe(deployment_df, use_container_width=True)
    
    # CI/CD Pipeline
    st.markdown("---")
    st.markdown("#### ðŸ”„ CI/CD Pipeline")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Pipeline Stages:**")
        st.code("""
        # .github/workflows/ml-pipeline.yml
        name: ML Pipeline
        
        on:
          push:
            branches: [main, develop]
          pull_request:
            branches: [main]
        
        jobs:
          test:
            runs-on: ubuntu-latest
            steps:
            - uses: actions/checkout@v3
            - name: Set up Python
              uses: actions/setup-python@v3
              with:
                python-version: 3.9
            - name: Install dependencies
              run: |
                pip install -r requirements.txt
                pip install pytest
            - name: Run tests
              run: pytest tests/
            - name: Data validation
              run: python scripts/validate_data.py
            - name: Model validation
              run: python scripts/validate_model.py
        
          deploy:
            needs: test
            if: github.ref == 'refs/heads/main'
            runs-on: ubuntu-latest
            steps:
            - name: Deploy to staging
              run: kubectl apply -f kubernetes/staging/
            - name: Run integration tests
              run: python scripts/integration_tests.py
            - name: Deploy to production
              run: kubectl apply -f kubernetes/production/
        """, language='yaml')
    
    with col2:
        st.markdown("**Quality Gates:**")
        st.markdown("""
        **Automated Checks:**
        - âœ… Unit test coverage > 80%
        - âœ… Data quality validation passes
        - âœ… Model performance > threshold
        - âœ… Security vulnerability scan
        - âœ… Code quality (pylint, black)
        
        **Manual Reviews:**
        - ðŸ‘¥ Code review approval
        - ðŸ“Š Model performance review
        - ðŸ” Business logic validation
        - ðŸ“‹ Documentation updates
        
        **Deployment Approval:**
        - ðŸŽ¯ Staging environment tests pass
        - ðŸ“ˆ Performance benchmarks met
        - ðŸ”’ Security clearance obtained
        - ðŸ“ Change management approved
        """)
    
    # Monitoring and alerting
    st.markdown("---")
    st.markdown("#### ðŸ“Š Monitoring & Alerting")
    
    monitoring_metrics = {
        'Category': [
            'Application Health',
            'Model Performance', 
            'Data Quality',
            'Business Metrics',
            'Infrastructure',
            'Security'
        ],
        'Key Metrics': [
            'Response time, Error rate, Throughput',
            'Accuracy, AUC, Prediction drift',
            'Data completeness, Schema drift',
            'Prediction volume, Intervention rate',
            'CPU, Memory, Disk usage',
            'Failed auth, Anomalous access'
        ],
        'Alert Thresholds': [
            'Response > 5s, Error > 1%',
            'Accuracy < 75%, AUC < 0.6',
            'Completeness < 95%',
            'Volume deviation > 20%',
            'CPU > 80%, Memory > 85%',
            'Failed auth > 10/min'
        ],
        'Tools': [
            'Prometheus + Grafana',
            'MLflow + Custom',
            'Great Expectations',
            'Custom Business Dashboard',
            'Prometheus + Node Exporter',
            'Security Information Event Management'
        ]
    }
    
    monitoring_df = pd.DataFrame(monitoring_metrics)
    st.dataframe(monitoring_df, use_container_width=True)
    
    # Incident response
    st.markdown("---")
    st.markdown("#### ðŸš¨ Incident Response Procedures")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        #### ðŸ”¥ Severity Levels
        
        **P0 - Critical (< 15 min)**
        - Service completely down
        - Data corruption/loss
        - Security breach
        
        **P1 - High (< 1 hour)**
        - Degraded performance
        - Model accuracy drop > 10%
        - Partial service failure
        
        **P2 - Medium (< 4 hours)**
        - Non-critical feature issues
        - Monitoring alerts
        - Performance degradation
        
        **P3 - Low (< 24 hours)**
        - Minor bugs
        - Documentation issues
        - Enhancement requests
        """)
    
    with col2:
        st.markdown("""
        #### ðŸ“‹ Response Procedures
        
        **Immediate Response (0-15 min):**
        1. Acknowledge alert
        2. Assess impact and severity
        3. Implement quick fixes/rollback
        4. Notify stakeholders
        
        **Investigation (15-60 min):**
        1. Gather logs and metrics
        2. Identify root cause
        3. Implement permanent fix
        4. Verify resolution
        
        **Post-Incident (1-24 hours):**
        1. Document incident details
        2. Conduct blameless postmortem
        3. Identify improvement actions
        4. Update procedures/monitoring
        """)

def show_api_documentation():
    """Show API documentation and examples."""
    
    st.markdown("### ðŸ“Š API Documentation")
    
    # API overview
    st.markdown("#### ðŸŒ REST API Endpoints")
    
    api_endpoints = {
        'Endpoint': [
            'POST /api/v1/predict',
            'POST /api/v1/predict/batch',
            'GET /api/v1/explain/{prediction_id}',
            'GET /api/v1/model/info',
            'GET /api/v1/health',
            'GET /api/v1/metrics'
        ],
        'Description': [
            'Single prediction request',
            'Batch prediction processing',
            'Explain prediction results',
            'Model metadata and version',
            'Service health check',
            'Performance metrics'
        ],
        'Auth Required': [
            'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes'
        ],
        'Rate Limit': [
            '1000/hour', '100/hour', '500/hour', 'None', 'None', '100/hour'
        ]
    }
    
    api_df = pd.DataFrame(api_endpoints)
    st.dataframe(api_df, use_container_width=True)
    
    # API examples
    st.markdown("---")
    st.markdown("#### ðŸ“ API Usage Examples")
    
    tab1, tab2, tab3 = st.tabs(["ðŸŽ¯ Single Prediction", "ðŸ“¦ Batch Prediction", "ðŸ” Explain Prediction"])
    
    with tab1:
        st.markdown("**Request Example:**")
        st.code("""
        curl -X POST "https://api.olist-ml.com/v1/predict" \\
             -H "Authorization: Bearer YOUR_API_KEY" \\
             -H "Content-Type: application/json" \\
             -d '{
               "order_id": "abc123",
               "total_price": 150.0,
               "total_items": 2,
               "freight_value": 15.0,
               "customer_state": "SP",
               "seller_state": "SP",
               "unique_sellers": 1,
               "product_weight": 1000,
               "product_photos": 3,
               "payment_installments": 1,
               "payment_type": "credit_card",
               "is_weekend": false,
               "is_holiday_season": false
             }'
        """, language='bash')
        
        st.markdown("**Response Example:**")
        st.code("""
        {
          "prediction_id": "pred_abc123_20231101",
          "order_id": "abc123",
          "satisfaction_probability": 0.73,
          "risk_level": "medium",
          "confidence": 0.85,
          "recommendations": [
            "Monitor delivery timeline closely",
            "Consider proactive customer communication"
          ],
          "timestamp": "2023-11-01T10:30:00Z",
          "model_version": "v1.2.0"
        }
        """, language='json')
    
    with tab2:
        st.markdown("**Batch Request Example:**")
        st.code("""
        curl -X POST "https://api.olist-ml.com/v1/predict/batch" \\
             -H "Authorization: Bearer YOUR_API_KEY" \\
             -H "Content-Type: application/json" \\
             -d '{
               "requests": [
                 {
                   "order_id": "order1",
                   "total_price": 120.0,
                   "total_items": 1,
                   // ... other features
                 },
                 {
                   "order_id": "order2", 
                   "total_price": 250.0,
                   "total_items": 3,
                   // ... other features
                 }
               ]
             }'
        """, language='bash')
        
        st.markdown("**Batch Response Example:**")
        st.code("""
        {
          "batch_id": "batch_20231101_001",
          "total_requests": 2,
          "processed": 2,
          "failed": 0,
          "results": [
            {
              "order_id": "order1",
              "satisfaction_probability": 0.82,
              "risk_level": "low"
            },
            {
              "order_id": "order2",
              "satisfaction_probability": 0.65,
              "risk_level": "medium"
            }
          ],
          "processing_time_ms": 150
        }
        """, language='json')
    
    with tab3:
        st.markdown("**Explanation Request:**")
        st.code("""
        curl -X GET "https://api.olist-ml.com/v1/explain/pred_abc123_20231101" \\
             -H "Authorization: Bearer YOUR_API_KEY"
        """, language='bash')
        
        st.markdown("**Explanation Response:**")
        st.code("""
        {
          "prediction_id": "pred_abc123_20231101",
          "feature_importance": [
            {
              "feature": "total_price",
              "importance": 0.15,
              "value": 150.0,
              "impact": "positive"
            },
            {
              "feature": "freight_ratio",
              "importance": -0.12,
              "value": 0.10,
              "impact": "negative"
            }
          ],
          "shap_values": {
            "base_value": 0.78,
            "feature_contributions": {
              "total_price": 0.05,
              "freight_ratio": -0.08,
              "customer_state": 0.02
            }
          },
          "decision_path": "Price positive â†’ Freight negative â†’ Geographic positive â†’ Final: 0.73"
        }
        """, language='json')
    
    # SDK examples
    st.markdown("---")
    st.markdown("#### ðŸ Python SDK Usage")
    
    st.code("""
    from olist_ml_client import OlistMLClient
    
    # Initialize client
    client = OlistMLClient(
        api_key="YOUR_API_KEY",
        base_url="https://api.olist-ml.com"
    )
    
    # Single prediction
    prediction = client.predict(
        order_id="abc123",
        total_price=150.0,
        total_items=2,
        freight_value=15.0,
        customer_state="SP",
        seller_state="SP",
        unique_sellers=1,
        product_weight=1000,
        product_photos=3,
        payment_installments=1,
        payment_type="credit_card",
        is_weekend=False,
        is_holiday_season=False
    )
    
    print(f"Satisfaction probability: {prediction.satisfaction_probability:.2f}")
    print(f"Risk level: {prediction.risk_level}")
    
    # Batch prediction
    orders = [
        {"order_id": "order1", "total_price": 120.0, ...},
        {"order_id": "order2", "total_price": 250.0, ...}
    ]
    
    batch_results = client.predict_batch(orders)
    
    for result in batch_results:
        print(f"Order {result.order_id}: {result.satisfaction_probability:.2f}")
    
    # Get explanation
    explanation = client.explain(prediction.prediction_id)
    
    for feature in explanation.feature_importance:
        print(f"{feature.name}: {feature.importance:.3f}")
    """, language='python')
    
    # Error handling
    st.markdown("---")
    st.markdown("#### âŒ Error Handling")
    
    error_codes = {
        'HTTP Code': [400, 401, 403, 429, 500, 503],
        'Error Type': [
            'Bad Request', 'Unauthorized', 'Forbidden',
            'Rate Limit', 'Internal Error', 'Service Unavailable'
        ],
        'Description': [
            'Invalid request format/data',
            'Missing or invalid API key',
            'Insufficient permissions',
            'Rate limit exceeded',
            'Server error occurred',
            'Service temporarily down'
        ],
        'Retry Strategy': [
            'Fix request and retry',
            'Check API key',
            'Contact support',
            'Exponential backoff',
            'Exponential backoff',
            'Exponential backoff'
        ]
    }
    
    error_df = pd.DataFrame(error_codes)
    st.dataframe(error_df, use_container_width=True)
    
    st.markdown("**Error Response Format:**")
    st.code("""
    {
      "error": {
        "code": "INVALID_INPUT",
        "message": "Missing required field: total_price",
        "details": {
          "field": "total_price",
          "expected_type": "float",
          "provided": null
        },
        "timestamp": "2023-11-01T10:30:00Z",
        "request_id": "req_abc123"
      }
    }
    """, language='json')